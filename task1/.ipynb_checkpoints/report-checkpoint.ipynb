{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c81417",
   "metadata": {},
   "source": [
    "# Task 1, solution by D. Uzhva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f76918",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Task statement\n",
    "\n",
    "Implement an approach described in the paper [\"Speaker Recognition from Raw Waveform with SincNet\"](https://arxiv.org/abs/1808.00158).\n",
    "Set up experiments and compare the results with those proposed and illustrated in the article.\n",
    "\n",
    "### Paper review\n",
    "\n",
    "The paper proposes SincNet CNN architecture\n",
    "SincNet is a one-dimensional convolutional neural network (CNN) used for speaker recognition.\n",
    "Compared to a classical 1-D CNN, SincNet differs in its first layer: convolutional kernels are limited to only sample values from the reference function \n",
    "$$\n",
    "    g[n, f_1, f_2] = 2 f_2 \\text{sinc}(2 \\pi f_2 n) - 2 f_1 \\text{sinc}(2 \\pi f_1 n),\n",
    "$$ \n",
    "where $f_1$ and $f_2$ are so-called cutoff frequencies and [$\\text{sinc}$](https://en.wikipedia.org/wiki/Sinc_function) function is defined as\n",
    "$$\n",
    "    \\text{sinc}(x) = \\frac{\\sin(x)}{x},\n",
    "$$ \n",
    "thus such layer is called SincConv.\n",
    "In the frequency domain, $\\text{sinc}$ functions represent band-pass filters, so that the proposed SincConv layer is believed to extract more meaningful features than filters with arbitrary values.\n",
    "Indeed, by applying band-pass filtering using $\\text{sinc}$ functions one may precisely extract pitch and formants from a sample of speech for further layers to learn such simplified (but at the same time more informative) features.\n",
    "In other words, band-pass filtering augments the data before the main CNN pipeline.\n",
    "\n",
    "More than that, by utilizing the described restriction, a SincConv layer would require only $2 F$ parameters to learn (two cutoff frequencies per filter), where $F$ is the number of SincConv filters in a layer.\n",
    "In comparison, a conventional convolution layer with $F$ filters of length $L$ would require $L \\cdot F >> 2 F$ parameters.\n",
    "With that being said, a SincConv layer may allow to increase overall network performance both during training and evaluation.\n",
    "\n",
    "### Research questions\n",
    "\n",
    "In order to verify the approach proposed in the paper, the following research questions are stated.\n",
    "\n",
    "1. Does a ScinConv layer increases the performance during training and validation?\n",
    "2. Does a SincConv layer improves accuracy of the CNN in speaker identification?\n",
    "\n",
    "The improvements are compared to a CNN with a basic first convolution layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5ea4d",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "\n",
    "Aiming to reproduce the method proposed in the article, a CNN training pipeline was prepared.\n",
    "The following packages were used:\n",
    "\n",
    "* Python 3\n",
    "* PyTorch\n",
    "* NumPy\n",
    "* Pandas\n",
    "* [SoundFile](https://pypi.org/project/SoundFile/)\n",
    "\n",
    "The SincConv layer with the network architecture are implemented in [model.py](model.py).\n",
    "Note that the implementation allows the SincConv layer to accept an input with multiple channels, in contrast to the single-channel version proposed in the [original paper](https://arxiv.org/abs/1808.00158).\n",
    "A model can be fed to the trainer ```train_model()``` function in [trainer.py](trainer.py), which also handles dataset split into train and validation parts.\n",
    "A dataset is loaded by a corresponding loader in [dataloader.py](dataloader.py).\n",
    "In [main.py](main.py), one can schedule training tasks for various configs defined in [cfg/](cfg/).\n",
    "Overall pipeline is outlined in [nn_pipeline.pdf](nn_pipeline.pdf):\n",
    "\n",
    "![](nn_pipeline.png)\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "For the series of experiments, the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) is exploited.\n",
    "This dataset is composed of 6300 spoken sentences stored in `.WAV` files, which correspond to 630 various speakers (10 sentences per speaker).\n",
    "The speakers can also be separated by their gender into two classes.\n",
    "With that being said, [timit_make_labels.py](timit_make_labels.py) allows to prepare a dataframe with label assignments for each sentence.\n",
    "Two labeling schemes are available: one obtains 630 classes, while the other extracts two gender classes.\n",
    "In this way, the same TIMIT dataset was used both for binary and multi-class speaker identification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8eeba",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cc809",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
